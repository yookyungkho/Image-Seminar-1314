---
description: 14기 서아라
---

# \[Lecture 16\] Adversarial Examples and Adversarial Training

안녕하세요 여러분:\)

오늘 리뷰할 강의의  Adversarial Examples and Adversarial Training입니다.

![](.gitbook/assets/image%20%28163%29.png)

아직 공부하는 단계이기도하고, 내용이 정말 많이 어려워서 이해가 부족한 부분이 많았습니다.

따라서 강의의 script 및 [https://www.programmersought.com/article/339084548/](https://www.programmersought.com/article/339084548/) 블로그의 글을 많이 참고하여 \(이해가 안가는 부분들은 거의 해석하다 시피..ㅠㅠ\)정리 하였음을 밝힙니다.

그래도 열심히 정리해보도록 하겠습니다. 도움이 되시길 바랍니다.

![](.gitbook/assets/image%20%28169%29.png)

오늘 강의의 목차입니다.

우선 저희는 adversarial examples가 무엇인지에 대하여 배울 예정입니다.

그런 다음 그것이 왜 일어나는 지에 대하여 알아볼 것입니다.

그리고 그것이machine learning systems에서 어떤 위협을 주는지, 방어 대책은 있는지, 마지막으로 adversary가 없는 상황에서 adversarial examples들이 어떻게 machine learning의 성능을 향상시키는 지 등에 대하여 알아볼 것입니다.

![](.gitbook/assets/image%20%28160%29.png)

Adversarial Examples를 설명할 때 가장 많이 쓰이는 이미지입니다.

사람은 너무 당연하게도 양 쪽의 사진이 모두 팬더라는 것을 압니다.

그러나 컴퓨터 같은 경우는 왼쪽의 사진에 가운데의 노이즈가 0.07의 비율로 포함된 오른쪽의 사진을 '긴코 원숭이'로 인식한다고 합니다.

놀랍게도, 팬더라고 인식했을 때\(57.7%\) 보다 더 높은 확신\(99.3%\)을 가지고 말입니다.

이와 같이 Adversarial attack이란 성능이 좋은 DNN을 이용한  Classifier들에 적대적 교란\(adversarial pertubation\)을 적용할 경우, 분류 알고리즘들이 쉽게 잘못된 분류를 할 수 있도록 하는 것을 의미합니다.

![](.gitbook/assets/image%20%28158%29.png)

이게 왜 일어나는 상황인가에 대하여 강사님의 동료분께서 실험\(?\)을 하는 과정에서 얻어낸 이미지들입니다.

한 박스에 붙어 있는 이미지가 저희가 보기에는 모두 같은 이미지처럼 보입니다.

하지만 해당 이미지들을 모두 numpy형태의 숫자로 바꾼 후, 차이를 보면 non-zero값이 나온다고 합니다.\(즉, 다른 이미지라는 의미입니다.\)

우리가 보기에 그리 큰 차이가 없어 보이는 이미지들인데,

신기하게도 한 박스 안에서 오른쪽 아래로 갈수록, 해당 이미지를 '비행기'에 가깝게 인식한다고 합니다.

우리는 보통 딥러닝이 비행기를 인식하는 방법은 '푸른 하늘과, 날개가 있을 때' 비행기라고 인식하는 것이라고 생각합니다.

물론 해당 내용도 맞겠지만, 실제로는 '그렇지만은 않다'라는 것입니다.

다른 이유로 해당 이미지를 '비행기'로 인식하기도 한다는 것입니다.

그리고, 이러한 이미지의 변형을 일으키는 것이 그리 어려운 것이 아니라고 합니다.

강사님의 동료 분도 단순히 네트웨크에 '비행기의 이미지를 만들어 달라'고 지시하기만 했고, 쉽게 비행기라고 착각할 수 있도록 하는 이미지를 만들어 주었다고 합니다.

생각보다 이러한 과정이 어렵지 않고 쉽다는 것은 쉽게 악용이 될 수 있다는 것과 같은 의미이기 때문에 문제입니다.

옆에 보이는 9로 이루어져 있는 이미지는, 딥러닝이 아닌 얕은 머신러닝 모델에 적용되어졌다고 합니다.

노란색 네모박스로 쳐진 숫자들은 왼쪽에서 오른쪽으로, 위에서 아래로 갈수록 차례로 0,1,2,...,9의 숫자를 나타내었다고 합니다.

마찬가지로 이미지의 숫자 9의 모양을 변형하지도 않았는데 다른 숫자로 인식하는 상황이 일어났고, 이는 linear model뿐만 아니라 다른 SVM이나 decision tree, nearest neighbors classifiers에도 해당하였다고 합니다.

![](.gitbook/assets/image%20%28156%29.png)

그동안 우리는 이러한 상황을 'over fitting'으로 설명해왔습니다.

딥러닝 모델이 너무 complicated하여 구불구불한 나머지, training set에 대하여는 잘 분류를 하였으나, test set에 대해서는 잘 분류를 하지 못하는 것이라는 의미입니다.

하지만, 만약 진짜로 overfitting의 문제라면, 각각의 adversarial examples에 대하여 다른 모델들도 random한 결과를 내야하는데, 그게 아니라 모두 같은 데이터에 대하여 같은 실수를 하고 있었습니다.

따라서 이것에 대하여 overfitting이 아닌 underfitting의 문제로 설명하기 시작하였습니다.

![](.gitbook/assets/image%20%28146%29.png)

그리고 모델이 underfitting이 된 이유는 모델이 부분적으로 'linear'하기 때문으로 설명하였습니다.

여러 레이어를 쌓기 때문에 완전히 linear한 것은 아니지만, ReLU, Maxout, Carefully tuned sigmoid, LSTM등을 사용하기 때문에 piecewise linear하다는 것입니다.

![](.gitbook/assets/image%20%28162%29.png)

그리고 우리가 기억할 것은, 우리가 adversarial examples를 만들 때, 이미지를 많이 변화를 주지 않고도 큰 변화를 만들어낼 수 있다는 것입니다.

'3'을 나타내는 이미지에 대하여 같은 크기의 L2 norm perturbation을 주었는데, '어떤 식으로 주느냐'에 따라서 7이다, 3\(그대로\)이다, 어떤 class에도 속하지 않는다 라는 3가지 정말 다른 변화를 이끌어내었습니다.

그러니까 결론적으로는 각 픽셀에 대한 작은 변화도 벡터 스페이스 상에서는 큰 변화를 이끌어낼 수 있다는 것입니다.\(제대로 된 이해인지는 모르겠습니다ㅠㅠ..\)

![](.gitbook/assets/image%20%28165%29.png)

adversarial example을 만들어내는 가장 빠른 방법은 cost의 gradient를 취하는 것이라 합니다.

이것을 Fast Gradient Sign Method라고 하고, 줄여서 FGSM라고 하는데 강의 내용만으로는 이해가 잘 가지 않아서, tensorflow의 설명을 참고하여 정리를 하도록 하겠습니다.

FGSM은 network의 gradient를 이용해 adversarial sample을 생성하는 기법이라고 합니다. 만약 모델의 입력이 image라면, imput image에 대한 cost function의 gradient를 계산하여 그 loss을 maximize하는 image를 생성합니다. 이처럼 새롭게 생성된 이미지를 적대적 이미지\(adversarial image\)라고 하고, 과정은 위에 슬라이드와 같이 정리할 수 있습니다.

![](.gitbook/assets/image%20%28161%29.png)

위 그림은 neural network decision boundary의 mapping function을 보여줍니다. 각 데이터 세트에 어떤 분류가 있어야하는지 보여주기 위해 2-dimentional input mapping의 intersection mapping을 작성하려고합니다.  위 그림의 각 작은 사각형은 CIFAR-10 분류기의 decision boundary의 mapping function이며 각 unit는 서로 다른 test sample에 해당합니다.  각 셀의 동작을 이해하는 데 도움이되는 범례가 왼쪽에 표시됩니다. 셀의 중간은 수정없이 original sample에 해당합니다. 왼쪽과 오른쪽은 FGSM attack direction을 나타내고 위쪽과 아래쪽은 FGSM에 orthogonal한 임의의 방향을 나타냅니다. 흰색 픽셀은 올바른 분류를 나타내고 다른 색상은 다른 범주를 나타냅니다. 셀의 왼쪽 절반이 기본적으로 쌍으로 나뉘고 오른쪽 절반은 다른 색이며 왼쪽과 오른쪽 경계 경계는 거의 선형임을 알 수 있습니다. 

이것은 FGSM이 방향을 인식한다는 것을 의미하며,이 방향으로 큰 내부 제품이 있으면 대결 샘플을 얻을 수 있습니다. 각 실제 샘플이 선형 결정 경계에 가깝고 경계를 대립 부분 공간으로 넘어갈 때 다른 모든 인접 지점이 샘플에 반대합니다. 즉, 올바른 방향을 찾는 한 특정 공간 좌표를 찾을 필요가 없습니다. 기울기 방향으로 큰 내적을 형성 할 수있는 방향, 방향 만 찾으면이 방향을 따라 조금 움직여 네트워크 모델을 속일 수 있는 것입니다.

또한 왼쪽과 오른쪽을 사용하여 intersection을 얻기 위해 축을 다시 참조합니다. FGSM을 사용하여 기울기 방향으로 큰 내적을 형성 할 수있는 두 번째 방향을 찾으면, 슬라이드의 오른쪽 그림처럼 두 축이 서로 마주 보게하고 대각선 방향 경계를 얻을 수 있습니다. 노이즈가 아닌 sample과 싸우는 것 중요합니다. confrontation sample에 많은 노이즈를 추가 할 수 있지만 여전히 antagonistic합니다. 

여기서 우리는 임의의 교차점을 만들고 두 축은 무작위로 선택된 방향입니다. 대부분이 올바르게 분류되어 있음을 알 수 있습니다. 일반적으로 샘플은 처음부터 정확하며 \(오류\) 노이즈를 추가해도 상황이 개선되지 않습니다. 

신기한 것이 있는데, 세 번째 행과 세 번째 열을 살펴보면, 노이즈로 인해 모델 분류가 잘못 될 수 있습니다. 특히 큰 노이즈는 더욱 그렇습니다. 또한 맨 위 행에서 일부 셀이 처음에 잘못 분류 된 것을 볼 수 있지만 노이즈로 인해 올바른 분류가 될 수 있습니다.

![](.gitbook/assets/image%20%28150%29.png)

이 그림에서 2 차원 상황을 살펴 보겠습니다. 

우리는 대결 영역이 평균 25 개의 차원을 가지고 있음을 발견했으며, 차원은 실제로 무작위 노이즈에 대해 샘플을 찾는 것이 얼마나 쉬운 지 알려줍니다. 대결 부분 공간의 차원은 마이그레이션의 특성과 밀접한 관련이 있습니다. 부분 공간의 차원이 클수록 두 모델의 부분 공간이 겹칠 가능성이 높습니다. 

두 개의 다른 모델이 있고 큰 대립 부분 공간이있는 경우 대립 샘플이 다른 모델로 마이그레이션 될 가능성이 큽니다. 대립 공간이 작 으면 부분 공간이 무작위이기 때문에 시스템 효과로 인해 두 모델이 정확히 동일한 부분 공간을 가지지 않는 한 샘플을 마이그레이션하기가 더 어렵습니다.

![](.gitbook/assets/image%20%28147%29.png)

위의 그림에서 말하는 clever hans의 사례와 유사하게, 사람들은 고도로 선형적인 모드가 훈련 데이터에 적응할 수 있고 심지어 테스트 데이터로 일반화 할 수 있다는 것을 알게되었고, 그들은 훈련 데이터의 동일한 분포를 마스터했습니다. 

상대가 의도적으로 모델을 속이기 위해 샘플을 생성하면 모델은 쉽게 속이게됩니다.

![](.gitbook/assets/image%20%28170%29.png)

위의 그림은 여러 가지 Gaussian noises를 보여주고 CIFAR-10 classifier로 실험되었다고 합니다. 분홍색 상자는 classifier가 무언가 있다고 느끼는 곳을 표시한 것입니다. 노란색 상자는 FGSM이있는 곳을 표시한 것이고, 성공적으로 모델이 비행기처럼 느끼게 하였다고 합니다.

![](.gitbook/assets/image%20%28151%29.png)

ppt의 왼쪽 상단을 보시면 Seaquest라는 Atari 게임을 볼 수 있으실 것입니다. 해당 게임은 original input pixel을 사용하여 FGSM 또는 선택 동작의 전략을 변경하기위한 maximum norm attack을 포함한 norm을 사용하여 disturbance를 계산합니다.

또 ppt의 오른쪽 아래를 보시면 RBF network를 볼 수 있으실 것입니다. RBF에 대한 문제에 대해서 말씀드리자면, 심층 RBF 네트워크는 훈련하기가 어렵고 샘플에 대한 문제를 해결하기 위해 성공적으로 훈련 할 수 있다는 점입니다. 이 모델은 매우 비선형이고 넓고 평평한 면적을 가지고 있기 때문에 적이 모델의 입력을 약간만 변경하여 손실 함수를 높이기 어렵다고 합니다.

![](.gitbook/assets/image%20%28166%29.png)

왼쪽의 anti-samples에 대해 주목해야 할 가장 중요한 점은 한 데이터 세트에서 다른 데이터 세트로, 한 모델에서 다른 데이터 세트로 일반화 할 수 있다는 것입니다. 위의 다이어그램은 두 개의 다른 훈련 세트에서 두 개의 다른 모델을 훈련합니다. 두 훈련 세트는 매우 작으며 MNIST에서 3과 7로 분류됩니다. 그래프의 왼쪽에있는 숫자를 사용하여 로지스틱 회귀 모델을 훈련하면 왼쪽 아래 모서리에 가중치를 부여하고 오른쪽 아래 모서리에 가중치를 부여하기 위해 오른쪽에있는 훈련을 사용합니다. 그래서 우리는 machine learning algorithm이 일반화 되었기 때문에 이와 비슷하게 보이는 두 개의 가중치 벡터를 배웠습니다. 

우리가 배우고 자하는 목적 함수는 우리가 훈련하는 데이터와 무관하며 우리가 어떤 훈련 샘플을 선택하는지는 중요하지 않습니다. 테스트 세트를 훈련 세트로 일반화하고 다른 훈련 세트가 유사한 결과를 얻을 수 있어야 합니다. 이것은 그들이 유사한 기능에 대해 배우는 것을 의미하므로 유사한 대결 샘플에서 약해질 것입니다.

우리는 다른 데이터 세트의 이동성뿐만 아니라 여러 다른 machine learning 기술의 이동성을 직접 측정 할 수 있습니다. 로지스틱 회귀로 구성된 회귀 샘플이 의사 결정 트리로 마이그레이션 할 확률이 87.4 %임을 발견했습니다. 

ppt의 오른쪽 그림을 보시면, 매트릭스의 어두운 블록은 많은 수의 마이그레이션을 나타냅니다. 이는 공격자가 왼쪽의 모델을 사용하여 오른쪽의 모델에 대해 카운터 샘플을 쉽게 구성 할 수 있음을 의미합니다.

![](.gitbook/assets/image%20%28157%29.png)

전체 프로세스는 다음과 같다고 합니다.

공격자가 직접 다룰 수 없는 모델을 속이려고한다고 가정 해봅시다. 그들은 훈련 모델의 구조도 모르고, 사용하는 알고리즘도 모르고, 공격 할 모델 매개 변수도 모릅니다. 그들이 할 수있는 것은 자신의 모델을 훈련시키는 것뿐입니다. 이들을 훈련시키는 방법에는 두 가지가 있습니다. 

하나는 자신의 훈련 세트에서 공격하려는 작업에 레이블을 지정하는 것입니다. 다른 하나는 자체 학습 세트를 구축하기에 충분하지 않지만 모델에 대한 액세스를 제한 할 수 있다는 것입니다. 즉, 모델에 입력을 제공하고 출력을 관찰 할 수 있습니다. 그런 다음 입력을 제공하고 출력을 관찰 한 다음 얻은 대상 모델이 모델에서 선택한 범주 레이블 만 출력에 제공하더라도이를 학습 세트로 사용할 수 있습니다.

![](.gitbook/assets/image%20%28171%29.png)

UC 버클리의 Dawn Song 외 다른 연구원분들은 여러 다른 모델을 통합 한 다음 경사 하강 법을 사용하여 대립 샘플을 검색하면 통합에서 각 모델을 속일 수 있음을 발견했습니다. 위의 그림은이 공격의 힘을 보여주고 있으며 의도적으로 마이그레이션 현상으로 이어질 때 매우 강력한 마이그레이션을 구성 할 수 있습니다.

![](.gitbook/assets/image%20%28149%29.png)

Nicolas 연구원께서는 마이그레이션 현상을 사용하여 Amazon과 Google이 만든 분류기를 속일 수 있음을 보여주었다고 합니다.

![](.gitbook/assets/image%20%28152%29.png)

휴대폰의 identification software를 사용하여 printed anti-sample images를 식별 한 결과 표적 인식 시스템이 실제로 deceived 되었다고 합니다. 카메라의 시스템이 샘플을 생성 한 모델과 동일하지 않아 모델이 마이그레이션 현상을 보냈음을 나타냅니다. 

이러한 방식으로 공격자가 모델의 에이전트에 대한 액세스 권한이없고 직접 접촉하지 않더라도 sample spoofing에 대해 physical proxy system을 효과적으로 사용할 수 있지만 개인이 받는 환경을 약간만 조정할 수 있다고 합니다.

모든 공격 메커니즘에서 방어는 거의 항상 실패한다고 합니다. 이 문제를 해결하기 위해 traditional regularization에 의존하는 것은 불가능합니다.

![](.gitbook/assets/image%20%28153%29.png)

모델을 생성하는 것만으로는이 문제를 해결할 수 없습니다. 입력 x의 실제 분포를 알아내는 것과 달리 레이블 y에 대해 주어진 입력 x의 정확한 사후 분포를 알아내는 것이 더 중요합니다. 그림의 왼쪽에는 두 개의 가우시안 분포를 혼합하여 얻은 바이 모달 분포가 있고 오른쪽에는 두 개의 라플라시안 분포가 혼합되어 있습니다. 입력 x에 대한 분포는 거의 구별 할 수 없으며 훈련 세트의 가능성 차이는 무시할 수 있지만 범주의 사후 분포는 매우 다릅니다. 왼쪽에는 분포가 끝날 ​​때 높은 수준의 신뢰도를 생성하는 로지스틱 회귀 분류 기가 있지만 여기에는 훈련 데이터가 없습니다.

이 방법은 사실적인 ImageNet 이미지를 생성하고 사후 분포를 올바르게 계산할 수있는 매우 풍부한 깊이 생성 모델을 설계하는 경우 효과적 일 것입니다. 그러나 현재 확률을 계산하는 것은 매우 어렵 기 때문에 일반적으로 사후 분포의 선형성을 매우 높게 만들기 위해 근사값으로 변경합니다.

![](.gitbook/assets/image%20%28148%29.png)

일반 근사 정리는 분류 함수가 원하는 모양이 무엇이든 상관없이 충분히 큰 신경망을 항상 표현할 수 있음을 알려줍니다. 이 기능을 얻기 위해 신경망을 훈련시킬 수 있는지 여부는 여전히 모든 사람이 해결할 수없는 문제이지만 최소한 한 쌍의 모양을 제공 할 수 있습니다.

anti-sample에 대한 훈련 만한다면? 각 훈련 세트의 입력 x에 대해 x + 공격이 여전히 원래 범주 태그에 해당 할 수 있기를 바랍니다. 이것은 약간의 효과가 있으며, 우리가 훈련하는 이런 종류의 공격에 점차적으로 저항 할 수 있습니다. 위의 녹색 곡선은 크게 떨어지지 않으며, 이는 샘플에 대한 테스트 세트의 오류율입니다. 파란색과 빨간색 곡선을 보면 적대적 샘플에 대한 훈련이 좋은 정규화 방법임을 알 수 있습니다.

![](.gitbook/assets/image%20%28167%29.png)

DNN 외에도 다른 모델은 confrontation training을 한다 해도 효과적이지 않을 가능성이 높다고 합니다.

![](.gitbook/assets/image%20%28168%29.png)

그러나 대립 훈련을하더라도 다른 카테고리에 대한 입력을 최적화하면 모델을 구성하기가 여전히 어렵습니다. 위의 그림에서 볼 수 있듯이 CIFAR-10의 트럭을 예로 들어, CIFAR-10 범주의 다른 모든 범주로 전환합니다. 우리는 중간 부분의 트럭이 새와 비슷하고 더 잘 맞는 유일한 트럭이라는 것을 발견했습니다. 이것은 대립 훈련을하더라도 우리는 여전히 문제를 해결하지 못한다는 것을 의미합니다.

적이 없는데도 기계 학습을 개선하기 위해 적대적 예제를 사용하는 방법은 무엇일까요?

![](.gitbook/assets/image%20%28164%29.png)

Adversarial Training을 할 때 우리는 모든 샘플의 label에 의존합니다. 만약 label이 없다면 어떻게 될까요? 놀랍게도 label 없이도 훈련 할 수 있으며 모델이 첫 번째 그림의 label을 예측하도록 할 수 있습니다. 한동안 훈련을했지만 모델이 완벽하게 훈련되지 않았다면 모델은 이것이 새라고 표시 될 수 있습니다. 새도 비행기 일 수 있습니다 \(모두 푸른 하늘이 있음\).

우리는 결정을 바꾸도록 고안된 방해 전파 defense를합니다. 우리는 네트워크가 트럭이나 혹은 그것과 비슷한 것으로 생각하게 만들고 싶습니다. 다음으로 카테고리 분포가 이전과 동일해야하는지, 즉 여전히 이것이 새 또는 비행기라고 생각하도록 훈련시킬 수 있습니다. 이 방법을 가상 대결 훈련이라고합니다.

![](.gitbook/assets/image%20%28154%29.png)

가상 대결 훈련을 사용하면 레이블이 지정된 데이터와 레이블이없는 데이터를 모두 사용하여 모델의 정확도를 향상시키는 준지도 학습을 달성 할 수 있습니다.

최적화 프로세스로 신경망을 사용할 때 여전히 많은 문제가 있습니다. 예를 들어, 우리는 매우 빠른 자동차를 만들고 싶습니다. 우리는 자동차의 청사진을 입력으로 받아들이고 자동차가 얼마나 빨리 운전할 수 있는지를 예측하는 신경망을 상상합니다. 우리는 신경망의 입력을 최적화하고 달릴 것으로 예상되는 가장 빠른 차의 청사진을 찾을 수 있습니다. 초고속 자동차. 하지만 안타깝게도 지금은 매우 빠른 자동차에 대한 청사진이 없습니다. 모델이 자동차가 매우 빠르다고 생각하도록 대결 샘플을 구성합니다. 표본에 대한 문제를 해결할 수 있다면 모델 기반 최적화 문제를 해결할 수 있습니다.

![](.gitbook/assets/image%20%28159%29.png)

마지막은 결론을 정리한 ppt입니다.



부족한 내용이었지만 읽어주셔서 감사합니다!:\)

&lt;Reference&gt;

[https://rain-bow.tistory.com/entry/Adversarial-Attack](https://rain-bow.tistory.com/entry/Adversarial-Attack)

 [https://www.programmersought.com/article/339084548/](https://www.programmersought.com/article/339084548/)   

{% file src=".gitbook/assets/cs231n\_16-review.pdf" caption="cs231n\_16강 review" %}

