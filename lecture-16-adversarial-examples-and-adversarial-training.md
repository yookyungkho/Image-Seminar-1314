---
description: 14기 서아라
---

# \[Lecture 16\] Adversarial Examples and Adversarial Training

안녕하세요, 투빅스 14기 서아라입니다.

오늘 리뷰할 강의의  Adversarial Examples and Adversarial Training입니다.

아직 공부하는 단계이기도하고, 부족한 부분이 많아서 다소 맞지 않는 부분도 있을까 염려됩니다.

그래도 열심히 정리해보도록 하겠습니다. 도움이 되시길 바랍니다.



오늘 강의의 목차입니다.

우선 저희는 adversarial examples가 무엇인지에 대하여 배울 예정입니다.

그런 다음 그것이 왜 일어나는 지에 대하여 알아볼 것입니다.

그리고 그것이machine learning systems에서 어떤 위협을 주는지, 방어 대책은 있는지, 마지막으로 adversary가 없는 상황에서 adversarial examples들이 어떻게 machine learning의 성능을 향상시키는 지 등에 대하여 알아볼 것입니다.

Adversarial Examples를 설명할 때 가장 많이 쓰이는 이미지입니다.

사람은 너무 당연하게도 양 쪽의 사진이 모두 팬더라는 것을 압니다.

그러나 컴퓨터 같은 경우는 왼쪽의 사진에 가운데의 노이즈가 0.07의 비율로 포함된 오른쪽의 사진을 '긴코 원숭이'로 인식한다고 합니다.

놀랍게도, 팬더라고 인식했을 때\(57.7%\) 보다 더 높은 확신\(99.3%\)을 가지고 말입니다.

이와 같이 Adversarial attack이란 성능이 좋은 DNN을 이용한  Classifier들에 적대적 교란\(adversarial pertubation\)을 적용할 경우, 분류 알고리즘들이 쉽게 잘못된 분류를 할 수 있도록 하는 것을 의미합니다.



--4p 나중에 정리--

이게 왜 일어나는 상황인가에 대하여 강사님의 동료분께서 실험\(?\)을 하는 과정에서 얻어낸 이미지들입니다.

한 박스에 붙어 있는 이미지가 저희가 보기에는 모두 같은 이미지처럼 보입니다.

하지만 해당 이미지들을 모두 numpy형태의 숫자로 바꾼 후, 차이를 보면 non-zero값이 나온다고 합니다.\(즉, 다른 이미지라는 의미입니다.\)

우리가 보기에 그리 큰 차이가 없어 보이는 이미지들인데,

신기하게도 한 박스 안에서 오른쪽 아래로 갈수록, 해당 이미지를 '비행기'에 가깝게 인식한다고 합니다.

우리는 보통 딥러닝이 비행기를 인식하는 방법은 '푸른 하늘과, 날개가 있을 때' 비행기라고 인식하는 것이라고 생각합니다.

물론 해당 내용도 맞겠지만, 실제로는 '그렇지만은 않다'라는 것입니다.

다른 이유로 해당 이미지를 '비행기'로 인식하기도 한다는 것입니다.

그리고, 이러한 이미지의 변형을 일으키는 것이 그리 어려운 것이 아니라고 합니다.

강사님의 동료 분도 단순히 네트웨크에 '비행기의 이미지를 만들어 달라'고 지시하기만 했고, 쉽게 비행기라고 착각할 수 있도록 하는 이미지를 만들어 주었다고 합니다.

생각보다 이러한 과정이 어렵지 않고 쉽다는 것은 쉽게 악용이 될 수 있다는 것과 같은 의미이기 때문에 문제입니다.



옆에 보이는 9로 이루어져 있는 이미지는, 딥러닝이 아닌 얕은 머신러닝 모델에 적용되어졌다고 합니다.

노란색 네모박스로 쳐진 숫자들은 왼쪽에서 오른쪽으로, 위에서 아래로 갈수록 차례로 0,1,2,...,9의 숫자를 나타내었다고 합니다.

마찬가지로 이미지의 숫자 9의 모양을 변형하지도 않았는데 다른 숫자로 인식하는 상황이 일어났고, 이는 linear model뿐만 아니라 다른 SVM이나 decision tree, nearest neighbors classifiers에도 해당하였다고 합니다.



그동안 우리는 이러한 상황을 'over fitting'으로 설명해왔습니다.

딥러닝 모델이 너무 complicated하여 구불구불한 나머지, training set에 대하여는 잘 분류를 하였으나, test set에 대해서는 잘 분류를 하지 못하는 것이라는 의미입니다.

하지만, 만약 진짜로 overfitting의 문제라면, 각각의 adversarial examples에 대하여 다른 모델들도 random한 결과를 내야하는데, 그게 아니라 모두 같은 데이터에 대하여 같은 실수를 하고 있었습니다.

따라서 이것에 대하여 overfitting이 아닌 underfitting의 문제로 설명하기 시작하였습니다.

그리고 모델이 underfitting이 된 이유는 모델이 부분적으로 'linear'하기 때문으로 설명하였습니다.

여러 레이어를 쌓기 때문에 완전히 linear한 것은 아니지만, ReLU, Maxout, Carefully tuned sigmoid, LSTM등을 사용하기 때문에 piecewise linear하다는 것입니다.



그리고 우리가 기억할 것은, 우리가 adversarial examples를 만들 때, 이미지를 많이 변화를 주지 않고도 큰 변화를 만들어낼 수 있다는 것입니다.

'3'을 나타내는 이미지에 대하여 같은 크기의 L2 norm perturbation을 주었는데, '어떤 식으로 주느냐'에 따라서 7이다, 3\(그대로\)이다, 어떤 class에도 속하지 않는다 라는 3가지 정말 다른 변화를 이끌어내었습니다.

그러니까 결론적으로는 각 픽셀에 대한 작은 변화도 벡터 스페이스 상에서는 큰 변화를 이끌어낼 수 있다는 것입니다.\(제대로 된 이해인지는 모르겠습니다ㅠㅠ..\)



adversarial example을 만들어내는 가장 빠른 방법은 cost의 gradient를 취하는 것이라 합니다.

이것을 Fast Gradient Sign Method라고 하고, 줄여서 FGSM라고 하는데 강의 내용만으로는 이해가 잘 가지 않아서, tensorflow의 설명을 참고하여 정리를 하도록 하겠습니다.

FGSM은 network의 gradient를 이용해 adversarial sample을 생성하는 기법이라고 합니다. 만약 모델의 입력이 image라면, imput image에 대한 cost function의 gradient를 계산하여 그 loss을 maximize하는 image를 생성합니다. 이처럼 새롭게 생성된 이미지를 적대적 이미지\(adversarial image\)라고 하고, 과정은 위에 슬라이드와 같이 정리할 수 있습니다.

 위 그림은 neural network decision boundary의 mapping function을 보여줍니다. 각 데이터 세트에 어떤 분류가 있어야하는지 보여주기 위해 2-dimentional input mapping의 intersection mapping을 작성하려고합니다. 

위 그림의 각 작은 사각형은 CIFAR-10 분류기의 decision boundary의 mapping function이며 각 unit는 서로 다른 test sample에 해당합니다. 

각 셀의 동작을 이해하는 데 도움이되는 범례가 왼쪽에 표시됩니다. 셀의 중간은 수정없이 original sample에 해당합니다.

 왼쪽과 오른쪽은 FGSM attack direction을 나타내고 위쪽과 아래쪽은 FGSM에 orthogonal한 임의의 방향을 나타냅니다. 

흰색 픽셀은 올바른 분류를 나타내고 다른 색상은 다른 범주를 나타냅니다. 

셀의 왼쪽 절반이 기본적으로 쌍으로 나뉘고 오른쪽 절반은 다른 색이며 왼쪽과 오른쪽 경계 경계는 거의 선형임을 알 수 있습니다. 

이것은 FGSM이 방향을 인식한다는 것을 의미하며,이 방향으로 큰 내부 제품이 있으면 대결 샘플을 얻을 수 있습니다. 

각 실제 샘플이 선형 결정 경계에 가깝고 경계를 대립 부분 공간으로 넘어갈 때 다른 모든 인접 지점이 샘플에 반대합니다.

즉, 올바른 방향을 찾는 한 특정 공간 좌표를 찾을 필요가 없습니다. 기울기 방향으로 큰 내적을 형성 할 수있는 방향, 방향 만 찾으면이 방향을 따라 조금 움직여 네트워크 모델을 속일 수 있는 것입니다.



또한 왼쪽과 오른쪽을 사용하여 intersection을 얻기 위해 축을 다시 참조합니다. 

FGSM을 사용하여 기울기 방향으로 큰 내적을 형성 할 수있는 두 번째 방향을 찾으면, 슬라이드의 오른쪽 그림처럼 두 축이 서로 마주 보게하고 대각선 방향 경계를 얻을 수 있습니다.



노이즈가 아닌 sample과 싸우는 것 중요합니다. confrontation sample에 많은 노이즈를 추가 할 수 있지만 여전히 antagonistic합니다. 

여기서 우리는 임의의 교차점을 만들고 두 축은 무작위로 선택된 방향입니다. 대부분이 올바르게 분류되어 있음을 알 수 있습니다. 

일반적으로 샘플은 처음부터 정확하며 \(오류\) 노이즈를 추가해도 상황이 개선되지 않습니다. 

신기한 것이 있는데, 세 번째 행과 세 번째 열을 살펴보면, 노이즈로 인해 모델 분류가 잘못 될 수 있습니다. 특히 큰 노이즈는 더욱 그렇습니다. 

또한 맨 위 행에서 일부 셀이 처음에 잘못 분류 된 것을 볼 수 있지만 노이즈로 인해 올바른 분류가 될 수 있습니다.

---



이 그림에서 2 차원 상황을 살펴 보겠습니다. 

우리는 대결 영역이 평균 25 개의 차원을 가지고 있음을 발견했으며, 차원은 실제로 무작위 노이즈에 대해 샘플을 찾는 것이 얼마나 쉬운 지 알려줍니다. 

대결 부분 공간의 차원은 마이그레이션의 특성과 밀접한 관련이 있습니다. 부분 공간의 차원이 클수록 두 모델의 부분 공간이 겹칠 가능성이 높습니다. 

두 개의 다른 모델이 있고 큰 대립 부분 공간이있는 경우 대립 샘플이 다른 모델로 마이그레이션 될 가능성이 큽니다. 

대립 공간이 작 으면 부분 공간이 무작위이기 때문에 시스템 효과로 인해 두 모델이 정확히 동일한 부분 공간을 가지지 않는 한 샘플을 마이그레이션하기가 더 어렵습니다.

--

위의 그림에서 말하는 clever hans의 사례와 유사하게, 사람들은 고도로 선형적인 모드가 훈련 데이터에 적응할 수 있고 심지어 테스트 데이터로 일반화 할 수 있다는 것을 알게되었고, 그들은 훈련 데이터의 동일한 분포를 마스터했습니다. 

상대가 의도적으로 모델을 속이기 위해 샘플을 생성하면 모델은 쉽게 속이게됩니다.

