---
description: 13기 강미경
---

# \[Paper Review 2\] You Only Look Once Unified, Real-Time Object Detection

![](.gitbook/assets/image%20%2849%29.png)

안녕하세요. You Only Look Once! YOLO-v1 리뷰입니다. 

![](.gitbook/assets/image%20%28130%29.png)

리뷰는 다음과 같은 흐름으로 진행됩니다. Object Detection과 2-stage 모델인 R-CNN 시리즈에 대한 간단한 리뷰, YOLO소개와 이를 이용한 실험, 그리고 YOLO 모델의 한계에 대해 말씀드리겠습니다.

![](.gitbook/assets/image%20%28128%29.png)

Object Detection이 무엇인지 기억하시나요? 바로 이미지 속 여러 물체들의 위치와, 그 물체가 어떤 것인지 찾아내는 테스크였습니다. 한 마디로 Multiobject에 대한 Localization과 Classification을 수행하는 것이라고 할 수 있습니다.

![](.gitbook/assets/image%20%28129%29.png)

이러한 기술은 어디에 이용될 수 있을까요? 우리가 아주 잘 알고 있는 자율주행 자동차에도 이용될 수 있고, CCTV를 실시간으로 탐지하여 응급상황, 범죄 등의 이상현상을 실시간으로 감지하는 데 사용될 수도 있습니다.

또 경기 동안 이뤄지는 선수들의 동선 등을 분석할 수도 있겠고,무인 점포 시스템에도 이용할 수 있습니다. 

실제로 아마존에서도 Amazon Go라는 이름으로 무인 점포를 출시하였다고 합니다. 아직은 완전 무인은 아니라고는 하지만 획기적인 시도였다고 생각합니다. 여기서는 사진에 나와있는 것처럼 진열장 안의 물체들의 위치 뿐만 아니라 사진은 없지만 고객이 들어와서 어떤 물건을 가지고 나가는지 까지 object detection으로 가능하다고 합니다.

![](.gitbook/assets/image%20%2893%29.png)

결론적으로 Object Detection 일상 생활에서 굉장히 많이 사용되기 때문에 잘 알아두어야 할 테스크라고 할 수 있습니다. 또, 앞선 예제들의 공통점이 있는데, 바로 ‘실시간\(real-time\)’이라는 것입니다. 

실시간으로 들어오는 영상을 처리하는데 가장 중요한 것은 바로 1초에 몇 개의 프레임을 처리할 수 있느냐에 대한 것, 바로 speed 입니다. 오늘 배울 YOLO가 바로 이 스피드에 초점을 맞춘 모델이기도 하죠.



![](.gitbook/assets/image%20%2827%29.png)

Object Detection 모델은 크게 1-stage와 2-stage 모 두 가지로 나눌 수 있습니다. 리가 지지난 시간에 배운 R-CNN 패시리즈가 바로 2-stage 모델에 속하고, 오늘 배울 YOLO는 1stage 모델입니다.

![](.gitbook/assets/image%20%2823%29.png)

먼저 2-stage Detector는 Region Proposal과 Classification을 순차적으로 하는 모델로, Localization을 통해 물체의 위치를 찾아내는 부분과, 그 물체가 어떤 클래스에 해당되는지 맞추는 부분이 따로 있습니다. Region을 찾고 그 찾은 Region에 대해 클래스를 분류하기 때문에 순차적으로 처리가 된다는 특징이 있었습니다.

  
이 2 stage 모델은 정확도는 비교적 잘 나오는데, 두 부분으로 나눠져 있기도 하고, 그 bounding box를 찾아내는 region proposal을 하는 것 자체가 느리기 때문에 전체적으로 스피드가 많이 떨어진다는 단점이 있었습니다.  


![](.gitbook/assets/image%20%2841%29.png)

1-stage 모델에서는 Region Proposal과 Classification이 동시에 이루어집니다. 이 1-stage 모델은 2-stage 기반 방식의 문제점이었던, region proposal에 시간이 오래 걸린다는 단점을 극복하고자 제안된 모델로, 굉장히 빨라서 실시간 영상도 곧잘 처리합니다. 단, 스피드에 투자를 하다보니 2-stage 모델보다 정확도는 조금 떨어진다는 특징이 있습니다. 

YOLO는 단 하나의 Convolution Nueral Network를 모델로 쓰는데, 이것의 출력인 Output Feature map에는 Region proposal로 예측된 바운딩 박스에 대한 정보도 가지고 있고, 그 박스 안에 물체가 있는지, 있다면 어떤 물체인지에 대한 정보도 다 가지고 있다고 합니다. 

YOLO로 들어가기 전에, 지난 시간\(Lecture 11\)에 배웠던 2-stage detector들이 워낙 중요한 내용이기 때문에 간단히 리뷰해보는 시간 갖겠습니다.

![](.gitbook/assets/image%20%2818%29.png)

**① R-CNN \(Region Proposal + CNN\)**  
Region Proposal 하는 부분과, CNN이 결합되었다고 해서 붙여진 이름입니다.  
한 마디로 전체 구조가, region proposal을 하는 부분, 그 결과들로 각각 cnn을 돌려 feature를 추출하는 부분, 추출된 피쳐를 바탕으로 classification과 Box Regression을 하는 부분으로 볼 수 있습니다. 

  
- **Region** **Proposal**

Selective Search를 이용합니다. 이는 객체와 주변간의 색감, 질의 차이 등을 파악해서 유사도를 기반으로 박스를 만들어나가는 알고리즘으로, 매우 좋지만 CPU 연산으로만 가능하기 때문에 느리다는 단점이 있었죠. 그리고 이 부분은 그저 알고리즘으로, 학습되는 네트워크가 아니었습니다.

  
**- Feature** **Extraction** 

Selective Search로부터 전달받은 박스 각각에 대해 CNN을 수행, Feature를 추출합니다. 박스가 2000개라면 2000개의 CNN 네트워크에 각각 보내야 하기 때문에 자연스럽게 전체 과정이 무겁고 느려질 수 밖에 없었으며 CNN이 고정크기의 입력을 받기 때문에 crop된 이미지를 전부 다 같은 크기로 맞춰주는 과정에서 비율 등의 정보의 손실이 일어난다는 단점이 있었죠.

  
- **Classification** **& Box** **regression**

이때는 cnn의 끝단에서 Fully Connected layer와 Softmax를 가지고 분류를 진행하지 않고, 아웃풋 피쳐맵을 가지고 머신러닝 알고리즘인 SVM에 넣어서 분류를 진행했습니다. 

이 박스가 맞는 박스냐, 아닌 박스냐와 해당 박스에 대한 Classification을 같이 진행하는 듯 합니다. 또 이 박스가 맞는 박스냐, 아닌 박스냐를 구별하기 위해서는 NMS\(Non-Maximum Suppression 알고리즘을 사용하였습니다. 

또 지도 학습이기 때문에 정답값이 존재합니다. 분류와 동시에 박스 좌표 및 너비와 폭의 정답인 ground truth와 현재의 바운딩 박스를 비교함으로써 박스의 위치를 세밀하게 조정해주었다고 합니다. \(Box Regression\)



![](.gitbook/assets/image%20%289%29.png)

이러한 R-CNN은 Selective Search에서 시간도 굉장히 오래 잡아먹고, 또 거기서 나온 애들을 각각 CNN에 넘기니까 굉장히 오래걸렸습니다. 또 모델도 CNN, SVM, Regression 부분까지 총 세개로 이루어져 있기 때문에 복잡하고, 이들끼리 서로 가중치를 공유하지 않기 때문에 back propagation이 안 되었다고 합니다.



![](.gitbook/assets/image%20%2839%29.png)

**② Fast R-CNN**  
R-CNN의 단점을 극복하고자 등장한 것이 Fast R-CNN입니다. 특징을 다음과 같이 크게 세 가지 정도로 잡을 수가 있습니다.  
- RoI Projection  
- RoI Pooling  
- SVM 대신에 Fully Connected Layer을 선택

![](.gitbook/assets/image%20%2832%29.png)

오른쪽 하단에 있는 예시 사진을 봅시다. 먼저 기존의 R-CNN과 똑같이 input image에 대해 Selective Search를 진행합니다. 그런데 얘는 그 결과로 나온 바운딩 박스들을 각각 CNN에 넣는 것이 아니라, 그 박스들의 좌표와 너비, 폭에 대한 정보를 가지고만 있죠.   
  
이 상태로 input image는 CNN을 통과해 피쳐맵을 추출합니다. 그러면 이때, RoI Projection이 일어나는데요, 가지고 있는 박스들을 출력된 피쳐맵에 투영하는 것입니다. Output feature map의 크기는 원본 이미지보다 작아졌겠죠? 그러면 그 작아진 비율도 고려해서 박스들이 output feature map에서 제 영역 부분만큼을 찾아가는 겁니다.  
  
이 과정에서 기존 R-CNN을 느리게 했던 원인중의 하나인 각 box마다 cnn을 수행하는 것이 없어지면서 한층 빠른 속도를 가질 수 있었습니다.   
  
그 다음은 이제 Bounding Box Regression을 통해 위치를 조정하고, 클래스를 에측하는 것이 남았습니다. 아까는 각 박스에 대해 수행한 cnn의 출력 피쳐맵을 input으로 하여 svm과 regression을 각각 수행했었는데 이제는 cnn이 아니라 FC layer를 통해서 박스의 피쳐를 추출하고, svm이 아니라 끝단의 softmax를 이용해서 multi classification을 진행합니다. regressor도 FC의 출력값을 가지고 진행합니다.  
  
그리고 이 Fully Connected Layer 또한 고정 크기의 벡터를 입력으로 받기 때문에 이를 맞춰주어야 합니다. 여기서는 RoI Pooling이라는 것을 이용해서, 제각기 다른 각 박스들의 크기를 하나로 맞춰춥니다. 



![](.gitbook/assets/image%20%28101%29.png)

다시 모델 구조를 전체적으로 정리한 내용은 다음과 같습니다. 

![](.gitbook/assets/image%20%286%29.png)

Fast R-CNN까지만 와도 되게 성능이 많이 향상되었습니다. 이전의 R-CNN보다 160배 빨라졌다고 합니다. 하지만 여전히 아쉬운 부분이 남아있었죠. 바로 여전히 region proposal을 selective search로 하고 있다는 점입니다.  
  
오른쪽 test time에서 보면, 파란색이 region proposal을 넣었을 때의 시간인데, 세 가지 모델에서 공통적으로 다 넣었을 때와 안넣었을 때 2초정도의 차이가 나는 것을 볼 수 있습니다.  
  
이는 Selective Search가 CPU를 사용하는 알고리즘이기 때문이었는데요, 2초라는 딜레이는 real time object detector에는 치명적일 수 있습니다. 다음 나올 Faster R-CNN은 이 부분 또한 네트워크로 교체해서 GPU로 연산도 가능하고, 학습도 가능하게끔 만들었습니다. 

![](.gitbook/assets/image%20%2824%29.png)

**③ Faster R-CNN**   
Faster R-CNN의 핵심은 빨간색 부분, Region Proposal Network 부분입니다. 기존 유사도 기반 알고리즘으로 행했던 region proposal을 어떻게 네트워크로 구현할 수 있었는지 간단히 살펴봅시다.  


![](.gitbook/assets/image%20%2891%29.png)

맨 처음 모델에 Input image가 들어오면, 통째로 CNN을 거칩니다. 그리고 거기서 출력된 Output Feature이 바로 RPN의 input이 됩니다. 또 RPN을 거쳐 나온 출력값들은 바로 Region들의 Sample입니다. 후보 바운딩 박스인 겁니다.  
그러면 이 sample들을 RoI pooling해서 크기를 맞춰준 후에, 동일하게 분류도 하고, Bounding Box Regression도 진행합니다. Fast R-CNN과 RPN 부분만 다르고 그 구조는 같은 것을 알 수 있습니다. 

![](.gitbook/assets/image%20%2812%29.png)

RPN에 대해 자세히 이해하려면 꽤 많은 말을 해야하기 때문에, 여기서는 간단히만 설명하고 넘어가겠습니다.   
일단 Input으로 Feature map이 들어옵니다. 여기서는 앵커의 개념이 나옵니다. 앵커 박스란 미리 정해놓은 box 후보들을 말합니다. 보통은 9개로 둔다고 하며, 3개의 다른 스케일, 3개의 다른 비율로 구성해 놓습니다.  
  
앵커에 대해 ground truth와 비교하여 IoU값을 구해 그 값이 0.7이상이거나 그 근방중 가장 높은 값인 박스에 Positive라벨을 부여합니다. 그 다음은 이 앵커박스에 대한 prediction을 진행하는데, 이는 두 가지 형태로 진행이 됩니다.   
첫 번째는 거기에 오브젝트가 있냐, 없냐에 대한 것, 즉 class가 2개인 분류기입니다. 또 다음은 regression layer로 두 개의 좌표와 너비, 폭, 즉 4개에 대해 regression을 진행하는 레이어입니다.   


![](.gitbook/assets/image%20%2872%29.png)

이렇게 앵커에 대한 prediction 값들을 각각 오브젝트의 유무와 그 박스 좌표에 대해서 예측하고나면, 이것과 IoU를 기반으로 라벨을 부여했던 정보와 같이 학습을 진행하게 됩니다. 그와 동시에 prediction된 앵커에 대한 정보들은 NMS 알고리즘을 바탕으로 추려져서 오른쪽으로 빠져나와 RoI Pooling을 거친 후에, 여태껏 그래왔던 것처럼 multi class 분류, 여기서는 오브젝트의 유무가 아니라 사전에 정의된 클래스들 중 어떤것인지, 즉 사람이냐 강아지냐 고양이냐에 대한 분류를 진행하고, 박스에 대한 리그레션을 진행하는 것입니다.   
  
쉽게 말하자면 RPN에서는 Feature map을 바탕으로 박스를 오브젝트가 있으면서 ground truth와 유사한 애들로 뽑도록 학습하고, 그런 네트워크에서 predicted된 후보 박스들은 NMS 알고리즘을 거쳐 적절히 sampling된 이후에, RoI 풀링을 거쳐 그 크기를 동일하게 맞춘 후 FC로 들어가 Classification과 Bbox Regression을 각각 수행한다. 라고 보면 될 것 같습니다.

![](.gitbook/assets/image%20%2870%29.png)

이런 Faster R-CNN은 드디어 2초의 장벽을 넘어서 0.2초라는 speed를 기록했다고 할 수 있습니다. 하지만 여전한 문제점은, 이렇게 test time이 빠른 Faster R-CNN도 실시간으로 처리되기엔 문제가 있었다는 것입니다. 1초당 처리할 수 있는 프레임의 개수인 FPS가 7로, 1초에 7개의 장면들을 처리할 수 있었지만, 이걸로는 좀 부족했던 거죠.   
  
그렇다면 어떻게 더 빠른 모델을 만들 수 있을까요? 1stage 디텍더들이 스피드를 겨냥한 모델들이며, 이것들이 해답이 될 수 있습니다. 그리고 오늘의 주제였던 그 1stage의 대표적인 디텍터인 YOLO를 지금부터 배우겠습니다.



![](.gitbook/assets/image%20%2817%29.png)



![](.gitbook/assets/image%20%2863%29.png)

YOLO-v1을 먼저 간단히 소개해보겠습니다.  

① Region Proposal과 Classification, Box Regression을 한번에 수행하는 1stage detector 입니다.

② 하나의 네트워크가 전체 이미지에서 bounding box를 찾고, 그 박스에 대한 class 확률도 계산합니다.

③ 이는 single network 이기 때문에 매우 빠르게 최적화 되고, 테스트 타임도 매우 짧아서 real time detector에 제격!

![](.gitbook/assets/image%20%2853%29.png)

구조를 간단히 보겠습니다. 먼저 448x448x3 size의 이미지가 input으로 들어옵니다. 그 다음 CNN네트워크가 나오는데, 논문에서는 GoogLeNet의 구조를 차용하여 ConvNet 4개 FCLayer 2개를 이용했다고 적혀있습니다. \(그림은 아니긴 합니다.\) 사실 레이어의 개수가 몇 개고, 어떤 모델을 차용했고는 지금은 크게 중요한 부분이 아니기도 합니다. 

![](.gitbook/assets/image%20%2813%29.png)

중요한 사실은, 어떤 형태이냐가 아니라 이것들의 역할이기 때문입니다. 이 CNN은 기본적으로 Feature를 추출하는 역할을 합니다. 그리고 이를 위해 ImageNet 데이터 셋을 이용해서 training을 시켰다고 합니다. YOLO 뿐만 아니라 대부분의 Feature extractor들은 ImageNet 데이터로 훈련시킨다고 합니다.

그리고 Feature Extraction을 하고 남은 몇 개의 layer에 대해서는 Object detect를 수행할 수 있도록 fine-tuning 한다고 합니다. 이 레이어에서 추출된 피쳐들에 대해서 바운딩 박스를 예측하고, 그 클래스를 예측하게 되는 것이죠. 이러한 과정을 거쳐 나온 최종 출력 피쳐맵은 다음과 같은 차원입니다. 



![](.gitbook/assets/image%20%2880%29.png)

예를 들면 7x7x30이라고 해볼까요? 이 각각의 숫자가 의미하는 것은 7x7은 그리드셀을 가로세로 7등분으로 만들었다는 의미가 되고, 30은 그 49개의 그리드 셀 중 하나가 가지고 있는 information vector의 길이가 됩니다. 30 안에는 bounding box에 대한 정보 뿐만 아니라, 해당 부분의 class probability에 대한 정보도 포함되어 있습니다. 

![](.gitbook/assets/image%20%2835%29.png)

![](.gitbook/assets/image%20%2865%29.png)

![](.gitbook/assets/image%20%283%29.png)

![](.gitbook/assets/image%20%2833%29.png)

![](.gitbook/assets/image%20%2882%29.png)

![](.gitbook/assets/image%20%28119%29.png)

![](.gitbook/assets/image%20%28120%29.png)

![](.gitbook/assets/image%20%2810%29.png)

![](.gitbook/assets/image%20%2887%29.png)

![](.gitbook/assets/image%20%28111%29.png)

![](.gitbook/assets/image%20%28114%29.png)

![](.gitbook/assets/image%20%2829%29.png)

[http://machinethink.net/blog/object-detection/](http://machinethink.net/blog/object-detection/)

\( Grid cell이 필요한 이유에 대해 아주 잘 설명되어 있는 링크입니다. 이 부분설명과 시각자료는 모두 이 링크를 참조하였음을 알려드립니다. \)

  
YOLO의 핵심에는 grid cell에 대한 개념이 빠질 수 없습니다. 이것이 즉, 네트워크 하나로 한 장의 사진 안에 있는 여러 개의 물체를 탐지할 수 있는 기술의 핵심입니다.   
Grid cell이 없는 경우의 예를 든다면 이를 훨씬 확실하게 이해할 수 있습니다.  
  
이런 사진이 있습니다. 이를 분류기로 분류하면 어떤 결과가 나올까요?  
Cat, dog에 대해 꽤 높은 클래스확률을 가지나 어느 것 하나도 확실하게 높지않는,  
분류기로서는 굉장히 두눈뜨고 못봐줄 결과이지만 이것이 분류기의 최선입니다.   
  
그러나 Object Detection을 이용하면 각 객체가 어디에 있고,   
그게 어떤 클래스인지도 예측할 수 있

![](.gitbook/assets/image%20%2859%29.png)

그리고 single object에 대한 것이라면, 이를 구현하는 것은 꽤 쉽습니다.

우리 데이터셋에 ground truth boxes가 있다면, 즉 박스와 클래스에 대한 라벨 값이 존재한다면 그냥 박스에 대한 x, y, w, h 네 가지 좌표를 모델에 추가해서 예측하면 됩니다. 이러면 모델의 output은 두 가지로 나타나게 됩니다. 

\(1\) 박스 안의 물체에 대한 그냥 우리가 일상적으로 하나의 이미지에 했었던 classification 

\(2\) box에 대한 Regression 값

그리고 학습 또한, 그냥 기존의 classification loss\(cross-entropy\)에다가 box regression에 대한 loss를 추가해주기만 하면 됩니다. 보통은 MSE를 사용하지요.

![](.gitbook/assets/image%20%2877%29.png)

그렇게 해서 학습한 결과는 꽤 잘 되게 나옵니다. 빨간 박스가 정답인 ground truth 결과라면 하늘색이 예측된 박스입니다. 그런데 이런 방법은 여러 개의 오브젝트가 들어가 있는 이미지에 대해서는 다음과 같이 Localization에 실패합니다. 이는 모델이 오직 하나의 바운딩 박스를 가지고, 두 개의 말을 예측하려고 하다보니까 발생하는 문제입니다. 

두 말 중 어느걸 골라야할 지 모르니, 그냥 정의롭게 두 말 사이의 값을 이용한 것 입니다. 그래서 박스의 크기 또한 두 말 사이의 값이 되고, 그 위치 또한 두 말 사이의 위치하게 됩니다. 

그렇다면 박스의 크기를 말의 개수만큼, 넣으면 어떨까요?

![](.gitbook/assets/image%20%28127%29.png)

두 개의 박스가 들어간다해도 결과는 거의 같습니다. 이 문제에 대한 원인은 ‘어떤 박스가 어떤 object를 전담할 지 정해지지 않았기 때문입니다’ 이 두 박스는 서로 협동하지 않습니다. 즉, 이 두 박스가 있을 때, 박스끼리 나는 왼쪽거 맡을게, 넌 오른쪽걸 맡아 하고 소통하지 않고 서로 독립적으로 행동한다는 것입니다. 때문에 하나를 넣든, 박스를 두 개를 넣든 그 결과가 거의 유사하게 나오는 거죠.   
  
두 박스 다 같은 생각을 하면서 가운데에 위치해 있는 겁니다. 이 문제를 해결하려면 다음과 같은 질문에 답할 수 있어야 합니다. 어떻게 하면 이 detector들이 하나의 물체에 대해서만 predict 하도록하면서 각각의 다른 detector들은 각기 다른 object에 집중할 수 있도록 할까?

즉, 각 물체에 대한 박스는 하나만 그러져야 하며, 하나의 박스는 하나의 물체를 전담해서 맡아야 한다는 것이죠. 그리고 1 stage detector들이 이것을 해결했습니다. 이들은 이미지를 균일한 grid cell로 나눈 이후에 각각의 bounding box detector들을 각 셀마다 할당했습니다. 텍터들의 포지션이 고정되어 있으므로, 이들은 오직 가까이에 위치한 오브젝트들만 탐지할 수 있습니다. 이렇게 하면 detector는 특정 위치에 있는 개체를 전문적으로 다루는 방법을 배울 수 있으며. 각 셀에 할당한 디텍터의 수를 늘림으로써 물체의 모양과 크기 또한 전문적으로 다루도록 할 수 있습니다. \(하나의 셀에 여러 개의 디텍터를 두는 것, YOLO-v1은 2개를 두었음\)

![](.gitbook/assets/image%20%2848%29.png)



![](.gitbook/assets/image%20%28121%29.png)

![](.gitbook/assets/image%20%2894%29.png)

![](.gitbook/assets/image%20%28104%29.png)

![](.gitbook/assets/image%20%2898%29.png)

![](.gitbook/assets/image%20%2889%29.png)

![](.gitbook/assets/image%20%2899%29.png)

![](.gitbook/assets/image%20%2858%29.png)

![](.gitbook/assets/image%20%2840%29.png)

![](.gitbook/assets/image%20%2822%29.png)

![](.gitbook/assets/image%20%2873%29.png)

![](.gitbook/assets/image%20%2884%29.png)

![](.gitbook/assets/image%20%2869%29.png)

![](.gitbook/assets/image%20%2814%29.png)

![](.gitbook/assets/image%20%2828%29.png)



![](.gitbook/assets/image%20%2897%29.png)

![](.gitbook/assets/image%20%2852%29.png)

![](.gitbook/assets/image%20%2885%29.png)

![](.gitbook/assets/image%20%2879%29.png)



![](.gitbook/assets/image%20%28116%29.png)

![](.gitbook/assets/image%20%28109%29.png)

![](.gitbook/assets/image%20%2850%29.png)

![](.gitbook/assets/image%20%2886%29.png)

![](.gitbook/assets/image%20%28100%29.png)



![](.gitbook/assets/image%20%28115%29.png)

![](.gitbook/assets/image%20%2820%29.png)







